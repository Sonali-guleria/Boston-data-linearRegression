{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475182857248_1346906012","id":"20160929-210057_1119681051","dateCreated":"2016-09-29T21:00:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2844","dateUpdated":"2016-09-29T21:20:49+0000","dateFinished":"2016-09-29T21:21:22+0000","dateStarted":"2016-09-29T21:20:49+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"Found 2 items\n-rw-r--r--   1 zeppelin hdfs     33.9 K 2016-09-29 21:21 /tmp/boston-data/boston_house.csv\n-rw-r--r--   1 zeppelin hdfs        280 2016-09-29 21:21 /tmp/boston-data/verification.csv\n"},"text":"%sh\n\n#remove existing copies of dataset from HDFS\nhadoop fs -rm -r -f /tmp/boston-data\nhadoop fs -mkdir /tmp/boston-data\n\n#put data into HDFS\nhadoop fs -put /tmp/boston/* /tmp/boston-data/\nhadoop fs -ls -h /tmp/boston-data/","focus":true},{"text":"%pyspark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nimport pyspark.mllib\nimport numpy as np\nimport pyspark.mllib.regression\nfrom pyspark.sql.functions import *\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.util import MLUtils\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.feature import StandardScaler, StandardScalerModel\nfrom pyspark.mllib.regression import LinearRegressionWithSGD, LinearRegressionModel\n\n\nhouses = sc.textFile(\"hdfs://sandbox.hortonworks.com/tmp/boston-data/boston_house.csv\")\n\n\nhouses = houses.map(lambda line: line.split(\",\"))\nhouseheader = houses.first() \nhouseframe = houses.filter(lambda line:line != househeader) \n\n\nhousefeatures = houseframe.map(lambda row: row[0:12])\n\nstandardizer = StandardScaler(withMean=True, withStd=True)\nscaler = standardizer.fit(housefeatures)\nscalerd = scaler.transform(housefeatures)\n\noutputs = houseframe.map(lambda row: row[13])\nInputOutput = outputs.zip(scalerd)\nInputOutput = InputOutput.map(lambda row: LabeledPoint(row[0],[row[1]]))\n\nprint \"train now\"\nlinearModel = LinearRegressionWithSGD.train(InputOutput, intercept=True)\nprint linearModel.weights\n\n\ntestdata = sc.textFile(\"hdfs://sandbox.hortonworks.com/tmp/boston-data/verification.csv\")\n\ntestdata1 = testdata.map(lambda line: line.split(\",\"))\ntestheader = testdata1.first()\ntestdata2 = testdata1.filter(lambda line:line != testheader)\n\n\n\ntestscalerd = scaler.transform(testdata2)\n\n\nprint \"done training\"\n\n\n\nmypred = linearModel.predict(testscalerd)\nmypred.saveAsTextFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/predicted1_results');\n","dateUpdated":"2016-09-29T21:39:48+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475182792541_1070994555","id":"20160929-205952_302992249","result":{"code":"ERROR","type":"TEXT","msg":"train now\n[-1.21942508007,0.678123215508,-0.540110224785,0.86782400039,-1.97648462739,4.4365473531,-1.30368217033,-2.9131900228,1.42288588412,-0.956358328568,-2.11159276319,1.26307805827]\ndone training\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1179.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1179.0 (TID 2241, sandbox.hortonworks.com): java.lang.IllegalArgumentException: requirement failed\n\tat scala.Predef$.require(Predef.scala:221)\n\tat org.apache.spark.mllib.feature.StandardScalerModel.transform(StandardScaler.scala:126)\n\tat org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)\n\tat org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:119)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:110)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:110)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1433)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1420)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1420)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:801)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1590)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:622)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1856)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1869)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1882)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalArgumentException: requirement failed\n\tat scala.Predef$.require(Predef.scala:221)\n\tat org.apache.spark.mllib.feature.StandardScalerModel.transform(StandardScaler.scala:126)\n\tat org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)\n\tat org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:119)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:110)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:110)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n', JavaObject id=o4421), <traceback object at 0x2319bd8>)"},"dateCreated":"2016-09-29T20:59:52+0000","dateStarted":"2016-09-29T21:35:11+0000","dateFinished":"2016-09-29T21:35:20+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2845","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475182862484_-2132763040","id":"20160929-210102_396273617","dateCreated":"2016-09-29T21:01:02+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2846"}],"name":"hw4","id":"2BYA6YDK4","angularObjects":{"2BW4S5NJ4:shared_process":[],"2BY1Y2HJG:shared_process":[],"2BVYH21ZJ:shared_process":[],"2BV6637D6:shared_process":[],"2BX5CPDSX:shared_process":[],"2BWHGGFZJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}